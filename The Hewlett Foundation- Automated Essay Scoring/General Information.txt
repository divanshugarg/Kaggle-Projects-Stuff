The William and Flora Hewlett Foundation (Hewlett) is sponsoring the Automated Student Assessment Prize (ASAP).  Hewlett is appealing to data scientists and machine learning specialists to help solve an important social problem.  We need fast, effective and affordable solutions for automated grading of student-written essays.

Hewlett is sponsoring the following prizes:

$60,000:  1st place
$30,000:  2nd place
$10,000:  3rd place
You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.  Your success depends upon how closely you can deliver scores to those of human expert graders.  While we believe that these financial incentives are important, we also intend to introduce top performers both to leading vendors in the industry and/or an established base of interested buyers.  Hewlett is opening the field of automated student assessment to you.  We want to induce a breakthrough that is both personally satisfying and game-changing for improving public education.

Today, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.  In this environment the need for more sophisticated and affordable options is vital.  For example, we know that essays are an important expression of academic achievement, but they are expensive and time consuming for states to grade them by hand.  So, we are frequently limited to multiple-choice standardized tests.  We believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.  We believe that you can help us pave the way towards a breakthrough.  ASAP is designed to achieve the following goals:

Challenge developers of automated student assessment systems to demonstrate their current capabilities.
Compare the efficacy and cost of automated scoring to that of human graders.
Reveal product capabilities to state departments of education and other key decision makers interested in adopting them.
The graded essays are selected according to specific data characteristics.  On average, each essay is approximately 150 to 550 words in length.  Some are more dependent upon source materials than others.  This range of essay type is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can match expert human graders for each essay.

You will be provided with training data for each essay prompt.  The number of training essays does vary.  For example, the lowest amount of training data is 1,190 essays, randomly selected from a total of 1,982.  The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.  Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers, but - keep in mind - that you will be predicting to the resolved score.  Also, please note that most essays are scored using a holistic scoring rubric.  However, one data set uses a trait scoring rubric.  The variability is intended to test the limits of your scoring engine’s capabilities.

Following a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.  However, you will notice that the rater and resolved score columns will be blank.  You will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site.

As part of the file that you will submit with your predictive scores, you will be asked to submit additional information.  We would like to understand both the time and capital that you’ve spent developing your engine, the profile of your team (or you as an individual if you are working alone) and the projected cost to implement your solution on a larger scale, along with any known limitations.  Basically, you will have the opportunity to present your case for who you are, why your model is commercially viable and to what extent you can use your model to satisfy the interests of potential buyers.  This other information will not be used to determine any prize rewards, and it is optional.  But, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work.

Also, please note that it is our intention to stage other follow-on ASAP phases in the months ahead.  We are starting with graded essays and will follow with new data:

Phase 1: Demonstration for long-form constructed response (essays);
Phase 2: Demonstration for short-form constructed response (short answers);
Phase 3: Demonstration for symbolic mathematical/logic reasoning (charts/graphs).
In every instance, we seek to drive innovation for new solutions to automated student assessment.  We hope that you will enjoy this process.  May the best model win!

